{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Method 1.ipynb","provenance":[],"collapsed_sections":["11afcjrijFkT","E09LWI3ljFkT"]}},"cells":[{"cell_type":"code","metadata":{"id":"9ZRyFNPNjFkQ"},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MZmGqSdOjFkQ"},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","from collections import defaultdict\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.linear_model import LinearRegression\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_squared_error, confusion_matrix, f1_score, hamming_loss, mean_absolute_error\n","from sklearn.metrics import multilabel_confusion_matrix, precision_score, recall_score, accuracy_score\n","import matplotlib.pyplot as plt\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","import itertools\n","import en_core_web_sm\n","import string\n","from nltk.tag import pos_tag\n","import gensim\n","from gensim.utils import simple_preprocess\n","from gensim.parsing.preprocessing import STOPWORDS\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","import nltk\n","from nltk.tokenize import sent_tokenize, word_tokenize \n","import warnings \n","from gensim.models import Word2Vec\n","import random\n","from scipy import stats"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZYFZgNpkjFkQ"},"source":["### word embedding"]},{"cell_type":"code","metadata":{"id":"BgjrgBmujFkQ","outputId":"9ae182f2-38db-4a79-dd61-a9165efd54a5"},"source":["nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     /Users/guandajiang/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"-k326XH2jFkR","outputId":"2779b1c5-cb9c-4bb7-8566-24248917dfb4"},"source":["stemmer = SnowballStemmer('english')\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to\n","[nltk_data]     /Users/guandajiang/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"6kRwEe3VjFkS"},"source":["def lemmatize_stemming(text):\n","    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n","\n","def preprocess(text):\n","    result = []\n","    for token in gensim.utils.simple_preprocess(text):\n","        if token not in gensim.parsing.preprocessing.STOPWORDS:\n","            result.append(lemmatize_stemming(token))\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Ep4ofQGjFkS"},"source":["df_1200 = pd.read_csv('Data/1200.csv')\n","y = df_1200['overall']\n","X = df_1200.iloc[:,1:7]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7QlU2BQvjFkS"},"source":["data = []\n","for text in range(df_1200.shape[0]):\n","    s = df_1200['reviewText'][text] \n","\n","    # Replaces escape character with space \n","    f = s.replace(\"\\n\", \" \") \n"," \n","\n","    # iterate through each sentence in the file \n","    for i in sent_tokenize(f): \n","        temp = [] \n","\n","        # tokenize the sentence into words \n","        for j in word_tokenize(i):\n","            j = lemmatize_stemming(j)\n","            temp.append(j.lower()) \n","\n","        data.append(temp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TOGAmL1gjFkS"},"source":["words = list(itertools.chain.from_iterable(data))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PQbV_tGsjFkS","outputId":"ef2a8271-0e3f-4a23-f671-eb427a14da1f"},"source":["len(words)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["367781"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"74yyogvdjFkS"},"source":["model1 = gensim.models.Word2Vec(data,min_count = 1,size = 100, window = 5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G7J1qNf7jFkS"},"source":["category = ['color','size', 'qualiti','comfort','price','materi']\n","similarity = defaultdict(list)\n","for i in category:\n","    for j in words:\n","        similarity[i].append(model1.similarity(i,j))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VVPkYTL6jFkS"},"source":["df_sim = pd.DataFrame(similarity)\n","df_sim.index = words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I6XE7DxgjFkS","outputId":"7507ada4-4d14-49e0-fb2b-25bd05784755"},"source":["df_sim"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>color</th>\n","      <th>size</th>\n","      <th>qualiti</th>\n","      <th>comfort</th>\n","      <th>price</th>\n","      <th>materi</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>good</th>\n","      <td>0.130726</td>\n","      <td>-0.021617</td>\n","      <td>0.601772</td>\n","      <td>0.666703</td>\n","      <td>0.558682</td>\n","      <td>0.599255</td>\n","    </tr>\n","    <tr>\n","      <th>price</th>\n","      <td>0.417415</td>\n","      <td>0.114226</td>\n","      <td>0.829234</td>\n","      <td>0.381946</td>\n","      <td>1.000000</td>\n","      <td>0.568117</td>\n","    </tr>\n","    <tr>\n","      <th>.</th>\n","      <td>0.142069</td>\n","      <td>0.141743</td>\n","      <td>0.233047</td>\n","      <td>0.223263</td>\n","      <td>0.136451</td>\n","      <td>0.306145</td>\n","    </tr>\n","    <tr>\n","      <th>comfort</th>\n","      <td>0.335236</td>\n","      <td>0.063729</td>\n","      <td>0.543135</td>\n","      <td>1.000000</td>\n","      <td>0.381946</td>\n","      <td>0.692009</td>\n","    </tr>\n","    <tr>\n","      <th>.</th>\n","      <td>0.142069</td>\n","      <td>0.141743</td>\n","      <td>0.233047</td>\n","      <td>0.223263</td>\n","      <td>0.136451</td>\n","      <td>0.306145</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>all</th>\n","      <td>0.178531</td>\n","      <td>-0.092414</td>\n","      <td>0.237093</td>\n","      <td>0.220900</td>\n","      <td>0.381846</td>\n","      <td>0.236102</td>\n","    </tr>\n","    <tr>\n","      <th>around</th>\n","      <td>-0.126808</td>\n","      <td>-0.010855</td>\n","      <td>-0.060926</td>\n","      <td>0.023871</td>\n","      <td>-0.026496</td>\n","      <td>0.168089</td>\n","    </tr>\n","    <tr>\n","      <th>great</th>\n","      <td>0.238396</td>\n","      <td>-0.080696</td>\n","      <td>0.526650</td>\n","      <td>0.633136</td>\n","      <td>0.482651</td>\n","      <td>0.482689</td>\n","    </tr>\n","    <tr>\n","      <th>hoodi</th>\n","      <td>0.415866</td>\n","      <td>0.244018</td>\n","      <td>0.455760</td>\n","      <td>0.229640</td>\n","      <td>0.459177</td>\n","      <td>0.405164</td>\n","    </tr>\n","    <tr>\n","      <th>.</th>\n","      <td>0.142069</td>\n","      <td>0.141743</td>\n","      <td>0.233047</td>\n","      <td>0.223263</td>\n","      <td>0.136451</td>\n","      <td>0.306145</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>367781 rows × 6 columns</p>\n","</div>"],"text/plain":["            color      size   qualiti   comfort     price    materi\n","good     0.130726 -0.021617  0.601772  0.666703  0.558682  0.599255\n","price    0.417415  0.114226  0.829234  0.381946  1.000000  0.568117\n",".        0.142069  0.141743  0.233047  0.223263  0.136451  0.306145\n","comfort  0.335236  0.063729  0.543135  1.000000  0.381946  0.692009\n",".        0.142069  0.141743  0.233047  0.223263  0.136451  0.306145\n","...           ...       ...       ...       ...       ...       ...\n","all      0.178531 -0.092414  0.237093  0.220900  0.381846  0.236102\n","around  -0.126808 -0.010855 -0.060926  0.023871 -0.026496  0.168089\n","great    0.238396 -0.080696  0.526650  0.633136  0.482651  0.482689\n","hoodi    0.415866  0.244018  0.455760  0.229640  0.459177  0.405164\n",".        0.142069  0.141743  0.233047  0.223263  0.136451  0.306145\n","\n","[367781 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"I-QJ2o14jFkS"},"source":["list_words = []\n","for i in category:\n","    similar_words = df_sim[i].loc[df_sim[i]>0.6].index\n","    similar_words = np.unique(similar_words)\n","    list_words.append(similar_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yCQGPswcjFkS"},"source":["color_words = list_words[0]\n","size_words = list_words[1]\n","quality_words = list_words[2]\n","comfort_words = list_words[3]\n","price_words = list_words[4]\n","material_words = list_words[5]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mg6VhCgHjFkS"},"source":["# the following sentiment words was collected by Minqing Hu and Bing Liu from\n","# https://github.com/shekhargulati/sentiment-analysis-python/blob/master/opinion-lexicon-English/positive-words.txt\n","f = open(\"data/positive_words.txt\", \"r\")\n","pos = f.read().split('\\n')\n","f = open(\"data/negative_words.txt\", \"r\")\n","neg = f.read().split('\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uvo4VRzcjFkS"},"source":["# the following code was learned from an tutorial from \n","#     Intellica.AI,\"Aspect-based Sentiment Analysis - Everthing You Wanted to Know\"\n","#     https://medium.com/@Intellica.AI/aspect-based-sentiment-analysis-everything-you-wanted-to-know-1be41572e238\n","\n","def feature_sentiment(sentence, pos, neg):\n","    '''\n","    input: dictionary and sentence\n","    function: appends dictionary with new features if the feature\n","              did not exist previously,then updates sentiment to\n","              each of the new or existing features\n","    output: updated dictionary\n","    '''\n","    sent_dict = dict()\n","    nlp = en_core_web_sm.load()\n","    sentence = nlp(sentence)\n","    opinion_words = neg + pos\n","    debug = 0\n","    \n","    for token in sentence:\n","        # check if the word is an opinion word, then assign sentiment\n","        if token.text in opinion_words:\n","            sentiment = 1 if token.text in pos else -1\n","            # if target is an adverb modifier (i.e. pretty, highly, etc.)\n","            # but happens to be an opinion word, ignore and pass\n","            if (token.dep_ == \"advmod\"):\n","                continue\n","            elif (token.dep_ == \"amod\"):\n","                sent_dict[token.head.text] = sentiment\n","                \n","            # for opinion words that are adjectives, adverbs, verbs...\n","            else:\n","                for child in token.children:\n","                    # if there's a adj modifier (i.e. very, pretty, etc.) add more weight to sentiment\n","                    # This could be better updated for modifiers that either positively or negatively emphasize\n","                    #if ((child.dep_ == \"amod\") or (child.dep_ == \"advmod\")) and (child.text in opinion_words):\n","                        #sentiment *= 1.5\n","                    # check for negation words and flip the sign of sentiment\n","                    if child.dep_ == \"neg\":\n","                        sentiment *= -1\n","                for child in token.children:\n","                    # if verb, check if there's a direct object\n","                    if (token.pos_ == \"VERB\") & (child.dep_ == \"dobj\"):                        \n","                        sent_dict[child.text] = sentiment\n","                        # check for conjugates (a AND b), then add both to dictionary\n","                        subchildren = []\n","                        conj = 0\n","                        for subchild in child.children:\n","                            if subchild.text == \"and\":\n","                                conj=1\n","                            if (conj == 1) and (subchild.text != \"and\"):\n","                                subchildren.append(subchild.text)\n","                                conj = 0\n","                        for subchild in subchildren:\n","                            sent_dict[subchild] = sentiment\n","\n","                # check for negation\n","                for child in token.head.children:\n","                    noun = \"\"\n","                    #if ((child.dep_ == \"amod\") or (child.dep_ == \"advmod\")) and (child.text in opinion_words):\n","                        #sentiment *= 1.5\n","                    # check for negation words and flip the sign of sentiment\n","                    if (child.dep_ == \"neg\"): \n","                        sentiment *= -1\n","                \n","                # check for nouns\n","                for child in token.head.children:\n","                    noun = \"\"\n","                    if (child.pos_ == \"NOUN\") and (child.text not in sent_dict):\n","                        noun = child.text\n","                        # Check for compound nouns\n","                        for subchild in child.children:\n","                            if subchild.dep_ == \"compound\":\n","                                noun = subchild.text + \" \" + noun\n","                        sent_dict[noun] = sentiment\n","                    debug += 1\n","    return sent_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6IhyGvm6jFkS"},"source":["df_1200['reviewText'] = df_1200['reviewText'].str.lower()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"avGT8i24jFkS"},"source":["sentiment = []\n","for i in df_1200['reviewText']:\n","    sentiment.append(feature_sentiment(i, pos, neg))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"JeD85NiWjFkS"},"source":["overall_rating = []\n","\n","for i in range(df_1200.shape[0]):\n","    sentiment_dict = {}\n","    \n","    if len(sentiment[i].keys()) != 0:\n","        \n","        for key in sentiment[i].keys():\n","            root = preprocess(key)\n","            \n","            if root in color_words:\n","                sentiment_dict['color'] = sentiment[i][key]\n","                \n","            if root in size_words:\n","                sentiment_dict['size'] = sentiment[i][key]\n","\n","            if root in quality_words:\n","                sentiment_dict['qualiti'] = sentiment[i][key]\n","\n","            if root in comfort_words:\n","                sentiment_dict['comfort'] = sentiment[i][key]\n","\n","            if root in price_words:\n","                sentiment_dict['price'] = sentiment[i][key]\n","\n","            if root in material_words:\n","                sentiment_dict['materi'] = sentiment[i][key]\n","\n","            else:\n","                sentiment_dict['NA'] = 0  \n","        \n","    else:\n","        sentiment_dict['NA'] = 0\n","    \n","    overall_rating.append(sentiment_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y4Ioe9vkjFkS","outputId":"47160228-4428-47b0-99de-d7d16a1ec35d"},"source":["rate_by_machine = pd.DataFrame(overall_rating)\n","rate_by_machine = rate_by_machine.drop(columns = 'NA')\n","rate_by_machine = rate_by_machine.fillna(0)\n","rate_by_machine"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>color</th>\n","      <th>comfort</th>\n","      <th>materi</th>\n","      <th>qualiti</th>\n","      <th>price</th>\n","      <th>size</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1186</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1187</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1188</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1189</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1190</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1191 rows × 6 columns</p>\n","</div>"],"text/plain":["      color  comfort  materi  qualiti  price  size\n","0       1.0      0.0     0.0      0.0    0.0   0.0\n","1       0.0      1.0     1.0      0.0    0.0   0.0\n","2       1.0      1.0     1.0      1.0    0.0   0.0\n","3       0.0      0.0     0.0      0.0    0.0   0.0\n","4       0.0      1.0     1.0      0.0    0.0   0.0\n","...     ...      ...     ...      ...    ...   ...\n","1186    0.0      0.0     0.0      1.0    1.0   0.0\n","1187    0.0      0.0     0.0      1.0    1.0   0.0\n","1188    0.0      0.0     0.0      0.0    0.0   0.0\n","1189    0.0      0.0     0.0      0.0    0.0   0.0\n","1190    0.0      0.0     0.0      0.0    0.0   0.0\n","\n","[1191 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"markdown","metadata":{"id":"11afcjrijFkT"},"source":["### Evaluation"]},{"cell_type":"code","metadata":{"id":"sY60fWO_jFkT"},"source":["def evaluate(y_true, y_pred, metric, average):\n","    result = defaultdict(list)\n","    for i in category:\n","        result[i].append(metric(y_true[i],y_pred[i], average = average))\n","    \n","    df_result = pd.DataFrame(result)\n","    \n","    return df_result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zqFoH7dGjFkT"},"source":["# random\n","random_dict = {}\n","for i in category:\n","    weight = df_1200[i].value_counts(normalize = True)\n","    random_score = random.choices([0,1,-1],\n","                                  weights = [weight.iloc[0],weight.iloc[1],weight.iloc[2]],k=df_1200.shape[0])\n","    random_dict[i] = random_score\n","    \n","df_random = pd.DataFrame(random_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JyHTDDaIjFkT"},"source":["**Precision**"]},{"cell_type":"code","metadata":{"id":"hxnNgaEZjFkT","outputId":"3c1e1678-6d36-41d3-cf9a-228a3d69680e"},"source":["prec = pd.concat([evaluate(X,rate_by_machine,precision_score, 'weighted'),\n","           evaluate(X,df_random,precision_score, 'weighted')]).T\n","prec.columns = ['Prediction','Random']\n","prec['difference'] = prec['Prediction'] - prec['Random']\n","prec"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Prediction</th>\n","      <th>Random</th>\n","      <th>difference</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>color</th>\n","      <td>0.891080</td>\n","      <td>0.717179</td>\n","      <td>0.173901</td>\n","    </tr>\n","    <tr>\n","      <th>size</th>\n","      <td>0.543663</td>\n","      <td>0.362626</td>\n","      <td>0.181037</td>\n","    </tr>\n","    <tr>\n","      <th>qualiti</th>\n","      <td>0.752166</td>\n","      <td>0.633160</td>\n","      <td>0.119007</td>\n","    </tr>\n","    <tr>\n","      <th>comfort</th>\n","      <td>0.658023</td>\n","      <td>0.546517</td>\n","      <td>0.111506</td>\n","    </tr>\n","    <tr>\n","      <th>price</th>\n","      <td>0.795687</td>\n","      <td>0.668947</td>\n","      <td>0.126740</td>\n","    </tr>\n","    <tr>\n","      <th>materi</th>\n","      <td>0.680011</td>\n","      <td>0.585982</td>\n","      <td>0.094029</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Prediction    Random  difference\n","color      0.891080  0.717179    0.173901\n","size       0.543663  0.362626    0.181037\n","qualiti    0.752166  0.633160    0.119007\n","comfort    0.658023  0.546517    0.111506\n","price      0.795687  0.668947    0.126740\n","materi     0.680011  0.585982    0.094029"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"markdown","metadata":{"id":"8fAvTPWIjFkT"},"source":["**Recall**"]},{"cell_type":"code","metadata":{"id":"ANhMNG4JjFkT","outputId":"61e257e4-f489-433f-9e03-1581b3f1e340"},"source":["recall = pd.concat([evaluate(X,rate_by_machine,recall_score, 'weighted'),\n","           evaluate(X,df_random,precision_score, 'weighted')]).T\n","recall.columns = ['Prediction','Random']\n","recall['difference'] = recall['Prediction'] - recall['Random']\n","recall"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Prediction</th>\n","      <th>Random</th>\n","      <th>difference</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>color</th>\n","      <td>0.901763</td>\n","      <td>0.717179</td>\n","      <td>0.184585</td>\n","    </tr>\n","    <tr>\n","      <th>size</th>\n","      <td>0.473552</td>\n","      <td>0.362626</td>\n","      <td>0.110926</td>\n","    </tr>\n","    <tr>\n","      <th>qualiti</th>\n","      <td>0.731318</td>\n","      <td>0.633160</td>\n","      <td>0.098159</td>\n","    </tr>\n","    <tr>\n","      <th>comfort</th>\n","      <td>0.680940</td>\n","      <td>0.546517</td>\n","      <td>0.134423</td>\n","    </tr>\n","    <tr>\n","      <th>price</th>\n","      <td>0.790092</td>\n","      <td>0.668947</td>\n","      <td>0.121145</td>\n","    </tr>\n","    <tr>\n","      <th>materi</th>\n","      <td>0.671704</td>\n","      <td>0.585982</td>\n","      <td>0.085722</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Prediction    Random  difference\n","color      0.901763  0.717179    0.184585\n","size       0.473552  0.362626    0.110926\n","qualiti    0.731318  0.633160    0.098159\n","comfort    0.680940  0.546517    0.134423\n","price      0.790092  0.668947    0.121145\n","materi     0.671704  0.585982    0.085722"]},"metadata":{"tags":[]},"execution_count":87}]},{"cell_type":"markdown","metadata":{"id":"BMOERcO7jFkT"},"source":["**F1 score**"]},{"cell_type":"code","metadata":{"id":"8RzNPdopjFkT","outputId":"4d5df038-f349-41a7-fac0-f5eb36f56f8d"},"source":["f1 = pd.concat([evaluate(X,rate_by_machine,f1_score, 'weighted'),\n","           evaluate(X,df_random,precision_score, 'weighted')]).T\n","f1.columns = ['Prediction','Random']\n","f1['difference'] = f1['Prediction'] - f1['Random']\n","f1"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Prediction</th>\n","      <th>Random</th>\n","      <th>difference</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>color</th>\n","      <td>0.890934</td>\n","      <td>0.717179</td>\n","      <td>0.173755</td>\n","    </tr>\n","    <tr>\n","      <th>size</th>\n","      <td>0.349154</td>\n","      <td>0.362626</td>\n","      <td>-0.013472</td>\n","    </tr>\n","    <tr>\n","      <th>qualiti</th>\n","      <td>0.733985</td>\n","      <td>0.633160</td>\n","      <td>0.100825</td>\n","    </tr>\n","    <tr>\n","      <th>comfort</th>\n","      <td>0.643395</td>\n","      <td>0.546517</td>\n","      <td>0.096878</td>\n","    </tr>\n","    <tr>\n","      <th>price</th>\n","      <td>0.792785</td>\n","      <td>0.668947</td>\n","      <td>0.123838</td>\n","    </tr>\n","    <tr>\n","      <th>materi</th>\n","      <td>0.669538</td>\n","      <td>0.585982</td>\n","      <td>0.083556</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Prediction    Random  difference\n","color      0.890934  0.717179    0.173755\n","size       0.349154  0.362626   -0.013472\n","qualiti    0.733985  0.633160    0.100825\n","comfort    0.643395  0.546517    0.096878\n","price      0.792785  0.668947    0.123838\n","materi     0.669538  0.585982    0.083556"]},"metadata":{"tags":[]},"execution_count":88}]},{"cell_type":"markdown","metadata":{"id":"wC4JB8-ljFkT"},"source":["**Accuracy Score**"]},{"cell_type":"code","metadata":{"id":"IDieN6GNjFkT"},"source":["accuracy_random = defaultdict(list)\n","accuracy_machine = defaultdict(list)\n","\n","for i in category:\n","    accuracy_random[i].append(accuracy_score(df_1200[i],df_random[i]))\n","    accuracy_machine[i].append(accuracy_score(df_1200[i],rate_by_machine[i]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DuwoAmtQjFkT","outputId":"024f9dd5-e655-4848-fd67-f4ac8f50019f"},"source":["acc = pd.concat([pd.DataFrame(accuracy_machine),\n","                 pd.DataFrame(accuracy_random)]).T\n","acc.columns = ['Prediction','Random']\n","acc['difference'] = acc['Prediction'] - acc['Random']\n","acc"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Prediction</th>\n","      <th>Random</th>\n","      <th>difference</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>color</th>\n","      <td>0.901763</td>\n","      <td>0.703610</td>\n","      <td>0.198153</td>\n","    </tr>\n","    <tr>\n","      <th>size</th>\n","      <td>0.473552</td>\n","      <td>0.362720</td>\n","      <td>0.110831</td>\n","    </tr>\n","    <tr>\n","      <th>qualiti</th>\n","      <td>0.731318</td>\n","      <td>0.628883</td>\n","      <td>0.102435</td>\n","    </tr>\n","    <tr>\n","      <th>comfort</th>\n","      <td>0.680940</td>\n","      <td>0.539043</td>\n","      <td>0.141898</td>\n","    </tr>\n","    <tr>\n","      <th>price</th>\n","      <td>0.790092</td>\n","      <td>0.652393</td>\n","      <td>0.137699</td>\n","    </tr>\n","    <tr>\n","      <th>materi</th>\n","      <td>0.671704</td>\n","      <td>0.593619</td>\n","      <td>0.078086</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Prediction    Random  difference\n","color      0.901763  0.703610    0.198153\n","size       0.473552  0.362720    0.110831\n","qualiti    0.731318  0.628883    0.102435\n","comfort    0.680940  0.539043    0.141898\n","price      0.790092  0.652393    0.137699\n","materi     0.671704  0.593619    0.078086"]},"metadata":{"tags":[]},"execution_count":100}]},{"cell_type":"markdown","metadata":{"id":"E09LWI3ljFkT"},"source":["### Significant Test"]},{"cell_type":"code","metadata":{"id":"ajrqIpG0jFkT","outputId":"dbbc9b18-402d-4fd1-c20c-c6f765359db8"},"source":["# using accuracy score as the skill estimate\n","stats.ttest_rel(acc['Prediction'],acc['Random'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Ttest_relResult(statistic=7.549701268243345, pvalue=0.0006460959427428329)"]},"metadata":{"tags":[]},"execution_count":117}]},{"cell_type":"code","metadata":{"id":"jQkyxoJ0jFkT","outputId":"5fd9e2c2-ea12-4469-ba11-b8a435265e08"},"source":["# using recall score as the skill estimate\n","stats.ttest_rel(recall['Prediction'],recall['Random'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Ttest_relResult(statistic=8.60613949743275, pvalue=0.0003494649511813355)"]},"metadata":{"tags":[]},"execution_count":119}]},{"cell_type":"code","metadata":{"id":"gyodY2i4jFkT","outputId":"430ce0fc-9165-4357-c061-7ff0b96ff4a7"},"source":["# using f1 score as the skill estimate\n","stats.ttest_rel(f1['Prediction'],f1['Random'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Ttest_relResult(statistic=3.7477640831721537, pvalue=0.013324396849565445)"]},"metadata":{"tags":[]},"execution_count":120}]},{"cell_type":"markdown","metadata":{"id":"NO59Z4iBjFkT"},"source":["**P-value is really small for metrics accuracy score, precision score and recall score,\\\n","and relatively small for the f1 score, but all of them are lower than the threshold 0.05,\\\n","therefore, we can reject the null hypothesis that there is no significant difference between trained model and random model.**"]}]}