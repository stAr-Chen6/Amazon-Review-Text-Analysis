{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/chenstar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/chenstar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# word embedding\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "  \n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# classify\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# evaluate\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sweatshirt_100words.csv') # 7000+ reviews\n",
    "category = ['color', 'size', 'qualiti', 'comfi', 'price', 'materi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    '''\n",
    "    Words are lemmatized — words in third person are changed to first person \n",
    "    and verbs in past and future tenses are changed into present.\n",
    "    Words are stemmed — words are reduced to their root form.\n",
    "    '''\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "data = []\n",
    "for text in range(df.shape[0]):\n",
    "    s = df['reviewText'][text] \n",
    "\n",
    "    # Replaces escape character with space \n",
    "    f = s.replace(\"\\n\", \" \") \n",
    " \n",
    "\n",
    "    # iterate through each sentence in the file \n",
    "    for i in sent_tokenize(f): \n",
    "        temp = [] \n",
    "\n",
    "        # tokenize the sentence into words \n",
    "        for j in word_tokenize(i):\n",
    "            j = lemmatize_stemming(j)\n",
    "            temp.append(j.lower()) \n",
    "\n",
    "        data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(itertools.chain.from_iterable(data))\n",
    "model1 = gensim.models.Word2Vec(data,min_count = 1,size = 100, window = 5) \n",
    "\n",
    "category = ['color', 'size', 'qualiti', 'comfi', 'price', 'materi']\n",
    "similarity = defaultdict(list)\n",
    "for i in category:\n",
    "    for j in words:\n",
    "        similarity[i].append(model1.similarity(i,j))\n",
    "        \n",
    "df_sim = pd.DataFrame(similarity)\n",
    "df_sim.index = words\n",
    "\n",
    "list_words = [] # including related words in each criteria\n",
    "for i in category:\n",
    "    words = df_sim[i].loc[df_sim[i]>0.6].index\n",
    "    words = np.unique(words)\n",
    "    list_words.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_words = [['array', 'bright', 'color', 'colour', 'depict', 'desrib', 'od',\n",
    "        'pictur', 'picture-', 'pricepoint', 'red', 'satur', 'select',\n",
    "        'shade', 'show', 'vibrant'], \n",
    "        ['2x', '2xl', '3x', '3xl', '4x', '4xl', 'bigger', 'bite', 'decid',\n",
    "        'did..', 'exchang', 'extra', 'l', 'larg', 'larger', 'loose-fit',\n",
    "        'm', 'may', 'medium', 'might', 'normal', 'order',\n",
    "        'price..although', 'run', 'should', 'sip', 'size', 'sizw',\n",
    "        'smaller', 'somewher', 'suggest', 'surer', 'swap', 'them.i',\n",
    "        'then', 'upmi', 'useal', 'usual', 'waaayyyyy', 'wil', 'x', 'xl',\n",
    "        'xtra', 'xxl', 'xxxxl', 'youth', 'zip'], ['.size', '105lbs', '11-buck', '23.00', '4-star', 'afford', 'amaz',\n",
    "        'asid', 'bargain', 'basic', 'beat', 'best', 'bethat', 'big/wid',\n",
    "        'boot', 'bunchi', 'cheap', 'choic', 'clip', 'colour', 'companion',\n",
    "        'contribut', 'damn', 'dat', 'deal', 'decent', 'diliveri',\n",
    "        'disappoint', 'disgust', 'downgrad', 'durabl', 'econo', 'elig',\n",
    "        'excel', 'exepcion', 'fabix', 'fabric', 'fabric..', 'fast', 'feel',\n",
    "        'find..', 'fit-wis', 'fitment', 'fleec', 'gidan', 'glue', 'good',\n",
    "        'good..', 'goof', 'headphon', 'heavyweight', 'hefti', 'high',\n",
    "        'high-end', 'impres', 'kind', 'lighter', 'lightweight', 'low',\n",
    "        'make', 'mater', 'materi', 'materiel', 'medium-weight', 'mic',\n",
    "        'midweight', 'money', 'none', 'ok', 'ok.', 'otherwis', 'outstand',\n",
    "        'overal', 'pace', 'pictur', 'pleasant', 'point', 'poor', 'price',\n",
    "        'pricepoint', 'product', 'qualiti', 'qualliti', 'realiti',\n",
    "        'reallti', 'reason', 'repent', 'reput', 'select', 'servic',\n",
    "        'shade', 'shirtshirt', 'simpl', 'siz3', 'sound', 'space/a',\n",
    "        'strength', 'sturdi', 'such', 'surviv', 'terrif', 'textur',\n",
    "        'theyr', 'tprice', 'trash', 'triangl', 'unbeat', 'valu',\n",
    "        'versatil', 'well-mad', 'worth'], ['.color', '.nice', '10-16', '105', '125lbs', '2996', '4-star',\n",
    "        '511', '80/20', '88-90lbs', '8oz', 'abd', 'above~', 'acuratt',\n",
    "        'afford', 'alright', 'amaz', 'anyway', 'asid', 'awesom', 'baggi',\n",
    "        'basic', 'beefi', 'besid', 'boxi', 'breath', 'breathabl', 'bright',\n",
    "        'broad-should', 'bulki', 'cardigan', 'cheap', 'chines', 'chocker',\n",
    "        'class', 'collect', 'colorfast', 'comfi', 'comfort', 'comfti',\n",
    "        'constuct', 'cotton/poli', 'cotton/polyest', 'cottoni', 'cozi',\n",
    "        'crop', 'cudd', 'curvi', 'cus', 'cute', 'darn', 'diamet',\n",
    "        'disproport', 'dumb', 'duper', 'durabl', 'duribl', 'easi', 'econo',\n",
    "        'extrem', 'fabric', 'fair', 'feel', 'fitment', 'fleec', 'flimsi',\n",
    "        'fluffi', 'fuzzi', 'gaudi', 'generous', 'gestur', 'gigant',\n",
    "        'glove', 'good', 'great', 'greatest', 'heavi', 'heavy-blend',\n",
    "        'heavyweight', 'heft', 'hell', 'hella', 'hoodie~it', 'hott',\n",
    "        'howev', 'huge', 'ideal', 'impress', 'incred', 'inexpens', 'insid',\n",
    "        'interior', 'kept', 'knit', 'light', 'light-weight',\n",
    "        'light/heavyweight', 'lightweight', 'litt', 'lol', 'loos',\n",
    "        'low-coast', 'materi', 'middleweight', 'midweight', 'mute', 'n',\n",
    "        'neither', 'nice', 'nonetheless', 'notabl', 'off-whit', 'ok',\n",
    "        'ok.', 'okay', 'otherwis', 'overal', 'overs', 'perfect', 'piti',\n",
    "        'pleanti', 'pleasant', 'pleased-it', 'plush', 'pretti', 'provid',\n",
    "        'puchas', 'quiet', 'quit', 'real', 'reali', 'realli', 'relat',\n",
    "        'relax', 'remind', 'resili', 'robberi', 'roomi', 's+', 'sane',\n",
    "        'shape', 'signatur', 'silki', 'simpl', 'simple/standard',\n",
    "        'simplic', 'siz3', 'sloppi', 'small/petit', 'smart', 'smooth',\n",
    "        'snugg', 'soft', 'solid', 'soo', 'sooo', 'soooooo', 'ssooo',\n",
    "        'stark', 'stiff', 'stiffer', 'strong', 'stun', 'sturdi', 'stylish',\n",
    "        'suffoc', 'sum', 'super', 'supper', 'surpric', 'surpris',\n",
    "        'sweatshirt/hoodi', 'taylor', 'theyr', 'thick', 'thin', 'toasti',\n",
    "        'tough', 'transpar', 'triangl', 'trim', 'ultra', 'unbeliev',\n",
    "        'uncomfort', 'uneven', 'unwear', 'usabl', 'utter', 'veri', 'vivid',\n",
    "        'warm', 'warm-but', 'warmth', 'washabl', 'wayyyi', 'weak',\n",
    "        'wearabl', 'weight', 'weird', 'wel', 'well-mad', 'wellmad', 'whoa',\n",
    "        'woild', 'wonder', 'worm', 'wow', 'yellow/gold'], \n",
    "        ['105lbs', '23.00', 'afford', 'asid', 'bargain', 'basic', 'beat',\n",
    "        'best', 'bethat', 'boot', 'cheap', 'choic', 'contribut', 'cost',\n",
    "        'dat', 'deal', 'decent', 'excel', 'fabric..', 'goof', 'headphon',\n",
    "        'look', 'low', 'mic', 'money', 'outstand', 'overal', 'pay',\n",
    "        'price', 'princ', 'product', 'qualiti', 'reason', 'repent',\n",
    "        'reput', 'servic', 'surviv', 'thank', 'unbeat', 'valu', 'worth'], ['125lb', '1990s..', '4-star', '8oz', 'also', 'although', 'amaz',\n",
    "        'asid', 'basic', 'besid', 'bethat', 'big/wid', 'breathabl',\n",
    "        'brilliant', 'broad-should', 'buhh', 'bulgi', 'bulki', 'bunchi',\n",
    "        'cheap', 'cheep', 'clip', 'comfi', 'comfort', 'cumbersom', 'darn',\n",
    "        'decent', 'deeper', 'deriv', 'descript', 'diamet', 'disproport',\n",
    "        'drawstr', 'duper', 'durabl', 'econo', 'either', 'elig', 'extrem',\n",
    "        'fabric', 'fair', 'farmstead', 'featur', 'feel', 'fitment',\n",
    "        'fleec', 'flimsi', 'fluffi', 'fond', 'freedom', 'fricken',\n",
    "        'garment', 'generous', 'gether', 'gigant', 'glue', 'good',\n",
    "        'greatest', 'headphon', 'heavier', 'heavy-blend', 'heavy/thick',\n",
    "        'heavyweight', 'heft', 'hefti', 'hood', 'howev', 'indulg',\n",
    "        'inferior', 'insid', 'inside..', 'irrit', 'is', 'itself', 'kind',\n",
    "        'light-weight', 'lighter', 'lightweight', 'line', 'low', 'make',\n",
    "        'materi', 'materiel', 'mic', 'middleweight', 'nice', 'none',\n",
    "        'notabl', 'nowher', 'ok', 'ok.', 'okay', 'otherwis', 'outstand',\n",
    "        'overal', 'pictur', 'pleasant', 'pleased-it', 'plush', 'point',\n",
    "        'poor', 'pretti', 'proplem', 'protrud', 'puchas', 'puff',\n",
    "        'qualiti', 'quit', 'roooooomi', 'scratchi', 'seem', 'shade',\n",
    "        'shrinkabl', 'side', 'silki', 'simple/standard', 'siz3', 'soft',\n",
    "        'space/a', 'staff', 'stark', 'stiff', 'strong', 'sturdi',\n",
    "        'substanti', 'super', 'surpris', 'sweater.thin', 'terrif',\n",
    "        'textur', 'theyr', 'thick', 'thicker', 'thin', 'thinner', 'though',\n",
    "        'tough', 'triangl', 'ultra', 'usabl', 'valu', 'veri', 'w/same',\n",
    "        'warm-but', 'warmer', 'warmth', 'weak', 'weight', 'well-mad',\n",
    "        'which', 'wide', 'wild', 'writ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nresult:\\n[array(['array', 'bright', 'color', 'colour', 'depict', 'desrib', 'od',\\n        'pictur', 'picture-', 'pricepoint', 'red', 'satur', 'select',\\n        'shade', 'show', 'vibrant'], dtype=object),\\n array(['2x', '2xl', '3x', '3xl', '4x', '4xl', 'bigger', 'bite', 'decid',\\n        'did..', 'exchang', 'extra', 'l', 'larg', 'larger', 'loose-fit',\\n        'm', 'may', 'medium', 'might', 'normal', 'order',\\n        'price..although', 'run', 'should', 'sip', 'size', 'sizw',\\n        'smaller', 'somewher', 'suggest', 'surer', 'swap', 'them.i',\\n        'then', 'upmi', 'useal', 'usual', 'waaayyyyy', 'wil', 'x', 'xl',\\n        'xtra', 'xxl', 'xxxxl', 'youth', 'zip'], dtype=object),\\n array(['.size', '105lbs', '11-buck', '23.00', '4-star', 'afford', 'amaz',\\n        'asid', 'bargain', 'basic', 'beat', 'best', 'bethat', 'big/wid',\\n        'boot', 'bunchi', 'cheap', 'choic', 'clip', 'colour', 'companion',\\n        'contribut', 'damn', 'dat', 'deal', 'decent', 'diliveri',\\n        'disappoint', 'disgust', 'downgrad', 'durabl', 'econo', 'elig',\\n        'excel', 'exepcion', 'fabix', 'fabric', 'fabric..', 'fast', 'feel',\\n        'find..', 'fit-wis', 'fitment', 'fleec', 'gidan', 'glue', 'good',\\n        'good..', 'goof', 'headphon', 'heavyweight', 'hefti', 'high',\\n        'high-end', 'impres', 'kind', 'lighter', 'lightweight', 'low',\\n        'make', 'mater', 'materi', 'materiel', 'medium-weight', 'mic',\\n        'midweight', 'money', 'none', 'ok', 'ok.', 'otherwis', 'outstand',\\n        'overal', 'pace', 'pictur', 'pleasant', 'point', 'poor', 'price',\\n        'pricepoint', 'product', 'qualiti', 'qualliti', 'realiti',\\n        'reallti', 'reason', 'repent', 'reput', 'select', 'servic',\\n        'shade', 'shirtshirt', 'simpl', 'siz3', 'sound', 'space/a',\\n        'strength', 'sturdi', 'such', 'surviv', 'terrif', 'textur',\\n        'theyr', 'tprice', 'trash', 'triangl', 'unbeat', 'valu',\\n        'versatil', 'well-mad', 'worth'], dtype=object),\\n array(['.color', '.nice', '10-16', '105', '125lbs', '2996', '4-star',\\n        '511', '80/20', '88-90lbs', '8oz', 'abd', 'above~', 'acuratt',\\n        'afford', 'alright', 'amaz', 'anyway', 'asid', 'awesom', 'baggi',\\n        'basic', 'beefi', 'besid', 'boxi', 'breath', 'breathabl', 'bright',\\n        'broad-should', 'bulki', 'cardigan', 'cheap', 'chines', 'chocker',\\n        'class', 'collect', 'colorfast', 'comfi', 'comfort', 'comfti',\\n        'constuct', 'cotton/poli', 'cotton/polyest', 'cottoni', 'cozi',\\n        'crop', 'cudd', 'curvi', 'cus', 'cute', 'darn', 'diamet',\\n        'disproport', 'dumb', 'duper', 'durabl', 'duribl', 'easi', 'econo',\\n        'extrem', 'fabric', 'fair', 'feel', 'fitment', 'fleec', 'flimsi',\\n        'fluffi', 'fuzzi', 'gaudi', 'generous', 'gestur', 'gigant',\\n        'glove', 'good', 'great', 'greatest', 'heavi', 'heavy-blend',\\n        'heavyweight', 'heft', 'hell', 'hella', 'hoodie~it', 'hott',\\n        'howev', 'huge', 'ideal', 'impress', 'incred', 'inexpens', 'insid',\\n        'interior', 'kept', 'knit', 'light', 'light-weight',\\n        'light/heavyweight', 'lightweight', 'litt', 'lol', 'loos',\\n        'low-coast', 'materi', 'middleweight', 'midweight', 'mute', 'n',\\n        'neither', 'nice', 'nonetheless', 'notabl', 'off-whit', 'ok',\\n        'ok.', 'okay', 'otherwis', 'overal', 'overs', 'perfect', 'piti',\\n        'pleanti', 'pleasant', 'pleased-it', 'plush', 'pretti', 'provid',\\n        'puchas', 'quiet', 'quit', 'real', 'reali', 'realli', 'relat',\\n        'relax', 'remind', 'resili', 'robberi', 'roomi', 's+', 'sane',\\n        'shape', 'signatur', 'silki', 'simpl', 'simple/standard',\\n        'simplic', 'siz3', 'sloppi', 'small/petit', 'smart', 'smooth',\\n        'snugg', 'soft', 'solid', 'soo', 'sooo', 'soooooo', 'ssooo',\\n        'stark', 'stiff', 'stiffer', 'strong', 'stun', 'sturdi', 'stylish',\\n        'suffoc', 'sum', 'super', 'supper', 'surpric', 'surpris',\\n        'sweatshirt/hoodi', 'taylor', 'theyr', 'thick', 'thin', 'toasti',\\n        'tough', 'transpar', 'triangl', 'trim', 'ultra', 'unbeliev',\\n        'uncomfort', 'uneven', 'unwear', 'usabl', 'utter', 'veri', 'vivid',\\n        'warm', 'warm-but', 'warmth', 'washabl', 'wayyyi', 'weak',\\n        'wearabl', 'weight', 'weird', 'wel', 'well-mad', 'wellmad', 'whoa',\\n        'woild', 'wonder', 'worm', 'wow', 'yellow/gold'], dtype=object),\\n array(['105lbs', '23.00', 'afford', 'asid', 'bargain', 'basic', 'beat',\\n        'best', 'bethat', 'boot', 'cheap', 'choic', 'contribut', 'cost',\\n        'dat', 'deal', 'decent', 'excel', 'fabric..', 'goof', 'headphon',\\n        'look', 'low', 'mic', 'money', 'outstand', 'overal', 'pay',\\n        'price', 'princ', 'product', 'qualiti', 'reason', 'repent',\\n        'reput', 'servic', 'surviv', 'thank', 'unbeat', 'valu', 'worth'],\\n       dtype=object),\\n array(['125lb', '1990s..', '4-star', '8oz', 'also', 'although', 'amaz',\\n        'asid', 'basic', 'besid', 'bethat', 'big/wid', 'breathabl',\\n        'brilliant', 'broad-should', 'buhh', 'bulgi', 'bulki', 'bunchi',\\n        'cheap', 'cheep', 'clip', 'comfi', 'comfort', 'cumbersom', 'darn',\\n        'decent', 'deeper', 'deriv', 'descript', 'diamet', 'disproport',\\n        'drawstr', 'duper', 'durabl', 'econo', 'either', 'elig', 'extrem',\\n        'fabric', 'fair', 'farmstead', 'featur', 'feel', 'fitment',\\n        'fleec', 'flimsi', 'fluffi', 'fond', 'freedom', 'fricken',\\n        'garment', 'generous', 'gether', 'gigant', 'glue', 'good',\\n        'greatest', 'headphon', 'heavier', 'heavy-blend', 'heavy/thick',\\n        'heavyweight', 'heft', 'hefti', 'hood', 'howev', 'indulg',\\n        'inferior', 'insid', 'inside..', 'irrit', 'is', 'itself', 'kind',\\n        'light-weight', 'lighter', 'lightweight', 'line', 'low', 'make',\\n        'materi', 'materiel', 'mic', 'middleweight', 'nice', 'none',\\n        'notabl', 'nowher', 'ok', 'ok.', 'okay', 'otherwis', 'outstand',\\n        'overal', 'pictur', 'pleasant', 'pleased-it', 'plush', 'point',\\n        'poor', 'pretti', 'proplem', 'protrud', 'puchas', 'puff',\\n        'qualiti', 'quit', 'roooooomi', 'scratchi', 'seem', 'shade',\\n        'shrinkabl', 'side', 'silki', 'simple/standard', 'siz3', 'soft',\\n        'space/a', 'staff', 'stark', 'stiff', 'strong', 'sturdi',\\n        'substanti', 'super', 'surpris', 'sweater.thin', 'terrif',\\n        'textur', 'theyr', 'thick', 'thicker', 'thin', 'thinner', 'though',\\n        'tough', 'triangl', 'ultra', 'usabl', 'valu', 'veri', 'w/same',\\n        'warm-but', 'warmer', 'warmth', 'weak', 'weight', 'well-mad',\\n        'which', 'wide', 'wild', 'writ'], dtype=object)]\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "result:\n",
    "[array(['array', 'bright', 'color', 'colour', 'depict', 'desrib', 'od',\n",
    "        'pictur', 'picture-', 'pricepoint', 'red', 'satur', 'select',\n",
    "        'shade', 'show', 'vibrant'], dtype=object),\n",
    " array(['2x', '2xl', '3x', '3xl', '4x', '4xl', 'bigger', 'bite', 'decid',\n",
    "        'did..', 'exchang', 'extra', 'l', 'larg', 'larger', 'loose-fit',\n",
    "        'm', 'may', 'medium', 'might', 'normal', 'order',\n",
    "        'price..although', 'run', 'should', 'sip', 'size', 'sizw',\n",
    "        'smaller', 'somewher', 'suggest', 'surer', 'swap', 'them.i',\n",
    "        'then', 'upmi', 'useal', 'usual', 'waaayyyyy', 'wil', 'x', 'xl',\n",
    "        'xtra', 'xxl', 'xxxxl', 'youth', 'zip'], dtype=object),\n",
    " array(['.size', '105lbs', '11-buck', '23.00', '4-star', 'afford', 'amaz',\n",
    "        'asid', 'bargain', 'basic', 'beat', 'best', 'bethat', 'big/wid',\n",
    "        'boot', 'bunchi', 'cheap', 'choic', 'clip', 'colour', 'companion',\n",
    "        'contribut', 'damn', 'dat', 'deal', 'decent', 'diliveri',\n",
    "        'disappoint', 'disgust', 'downgrad', 'durabl', 'econo', 'elig',\n",
    "        'excel', 'exepcion', 'fabix', 'fabric', 'fabric..', 'fast', 'feel',\n",
    "        'find..', 'fit-wis', 'fitment', 'fleec', 'gidan', 'glue', 'good',\n",
    "        'good..', 'goof', 'headphon', 'heavyweight', 'hefti', 'high',\n",
    "        'high-end', 'impres', 'kind', 'lighter', 'lightweight', 'low',\n",
    "        'make', 'mater', 'materi', 'materiel', 'medium-weight', 'mic',\n",
    "        'midweight', 'money', 'none', 'ok', 'ok.', 'otherwis', 'outstand',\n",
    "        'overal', 'pace', 'pictur', 'pleasant', 'point', 'poor', 'price',\n",
    "        'pricepoint', 'product', 'qualiti', 'qualliti', 'realiti',\n",
    "        'reallti', 'reason', 'repent', 'reput', 'select', 'servic',\n",
    "        'shade', 'shirtshirt', 'simpl', 'siz3', 'sound', 'space/a',\n",
    "        'strength', 'sturdi', 'such', 'surviv', 'terrif', 'textur',\n",
    "        'theyr', 'tprice', 'trash', 'triangl', 'unbeat', 'valu',\n",
    "        'versatil', 'well-mad', 'worth'], dtype=object),\n",
    " array(['.color', '.nice', '10-16', '105', '125lbs', '2996', '4-star',\n",
    "        '511', '80/20', '88-90lbs', '8oz', 'abd', 'above~', 'acuratt',\n",
    "        'afford', 'alright', 'amaz', 'anyway', 'asid', 'awesom', 'baggi',\n",
    "        'basic', 'beefi', 'besid', 'boxi', 'breath', 'breathabl', 'bright',\n",
    "        'broad-should', 'bulki', 'cardigan', 'cheap', 'chines', 'chocker',\n",
    "        'class', 'collect', 'colorfast', 'comfi', 'comfort', 'comfti',\n",
    "        'constuct', 'cotton/poli', 'cotton/polyest', 'cottoni', 'cozi',\n",
    "        'crop', 'cudd', 'curvi', 'cus', 'cute', 'darn', 'diamet',\n",
    "        'disproport', 'dumb', 'duper', 'durabl', 'duribl', 'easi', 'econo',\n",
    "        'extrem', 'fabric', 'fair', 'feel', 'fitment', 'fleec', 'flimsi',\n",
    "        'fluffi', 'fuzzi', 'gaudi', 'generous', 'gestur', 'gigant',\n",
    "        'glove', 'good', 'great', 'greatest', 'heavi', 'heavy-blend',\n",
    "        'heavyweight', 'heft', 'hell', 'hella', 'hoodie~it', 'hott',\n",
    "        'howev', 'huge', 'ideal', 'impress', 'incred', 'inexpens', 'insid',\n",
    "        'interior', 'kept', 'knit', 'light', 'light-weight',\n",
    "        'light/heavyweight', 'lightweight', 'litt', 'lol', 'loos',\n",
    "        'low-coast', 'materi', 'middleweight', 'midweight', 'mute', 'n',\n",
    "        'neither', 'nice', 'nonetheless', 'notabl', 'off-whit', 'ok',\n",
    "        'ok.', 'okay', 'otherwis', 'overal', 'overs', 'perfect', 'piti',\n",
    "        'pleanti', 'pleasant', 'pleased-it', 'plush', 'pretti', 'provid',\n",
    "        'puchas', 'quiet', 'quit', 'real', 'reali', 'realli', 'relat',\n",
    "        'relax', 'remind', 'resili', 'robberi', 'roomi', 's+', 'sane',\n",
    "        'shape', 'signatur', 'silki', 'simpl', 'simple/standard',\n",
    "        'simplic', 'siz3', 'sloppi', 'small/petit', 'smart', 'smooth',\n",
    "        'snugg', 'soft', 'solid', 'soo', 'sooo', 'soooooo', 'ssooo',\n",
    "        'stark', 'stiff', 'stiffer', 'strong', 'stun', 'sturdi', 'stylish',\n",
    "        'suffoc', 'sum', 'super', 'supper', 'surpric', 'surpris',\n",
    "        'sweatshirt/hoodi', 'taylor', 'theyr', 'thick', 'thin', 'toasti',\n",
    "        'tough', 'transpar', 'triangl', 'trim', 'ultra', 'unbeliev',\n",
    "        'uncomfort', 'uneven', 'unwear', 'usabl', 'utter', 'veri', 'vivid',\n",
    "        'warm', 'warm-but', 'warmth', 'washabl', 'wayyyi', 'weak',\n",
    "        'wearabl', 'weight', 'weird', 'wel', 'well-mad', 'wellmad', 'whoa',\n",
    "        'woild', 'wonder', 'worm', 'wow', 'yellow/gold'], dtype=object),\n",
    " array(['105lbs', '23.00', 'afford', 'asid', 'bargain', 'basic', 'beat',\n",
    "        'best', 'bethat', 'boot', 'cheap', 'choic', 'contribut', 'cost',\n",
    "        'dat', 'deal', 'decent', 'excel', 'fabric..', 'goof', 'headphon',\n",
    "        'look', 'low', 'mic', 'money', 'outstand', 'overal', 'pay',\n",
    "        'price', 'princ', 'product', 'qualiti', 'reason', 'repent',\n",
    "        'reput', 'servic', 'surviv', 'thank', 'unbeat', 'valu', 'worth'],\n",
    "       dtype=object),\n",
    " array(['125lb', '1990s..', '4-star', '8oz', 'also', 'although', 'amaz',\n",
    "        'asid', 'basic', 'besid', 'bethat', 'big/wid', 'breathabl',\n",
    "        'brilliant', 'broad-should', 'buhh', 'bulgi', 'bulki', 'bunchi',\n",
    "        'cheap', 'cheep', 'clip', 'comfi', 'comfort', 'cumbersom', 'darn',\n",
    "        'decent', 'deeper', 'deriv', 'descript', 'diamet', 'disproport',\n",
    "        'drawstr', 'duper', 'durabl', 'econo', 'either', 'elig', 'extrem',\n",
    "        'fabric', 'fair', 'farmstead', 'featur', 'feel', 'fitment',\n",
    "        'fleec', 'flimsi', 'fluffi', 'fond', 'freedom', 'fricken',\n",
    "        'garment', 'generous', 'gether', 'gigant', 'glue', 'good',\n",
    "        'greatest', 'headphon', 'heavier', 'heavy-blend', 'heavy/thick',\n",
    "        'heavyweight', 'heft', 'hefti', 'hood', 'howev', 'indulg',\n",
    "        'inferior', 'insid', 'inside..', 'irrit', 'is', 'itself', 'kind',\n",
    "        'light-weight', 'lighter', 'lightweight', 'line', 'low', 'make',\n",
    "        'materi', 'materiel', 'mic', 'middleweight', 'nice', 'none',\n",
    "        'notabl', 'nowher', 'ok', 'ok.', 'okay', 'otherwis', 'outstand',\n",
    "        'overal', 'pictur', 'pleasant', 'pleased-it', 'plush', 'point',\n",
    "        'poor', 'pretti', 'proplem', 'protrud', 'puchas', 'puff',\n",
    "        'qualiti', 'quit', 'roooooomi', 'scratchi', 'seem', 'shade',\n",
    "        'shrinkabl', 'side', 'silki', 'simple/standard', 'siz3', 'soft',\n",
    "        'space/a', 'staff', 'stark', 'stiff', 'strong', 'sturdi',\n",
    "        'substanti', 'super', 'surpris', 'sweater.thin', 'terrif',\n",
    "        'textur', 'theyr', 'thick', 'thicker', 'thin', 'thinner', 'though',\n",
    "        'tough', 'triangl', 'ultra', 'usabl', 'valu', 'veri', 'w/same',\n",
    "        'warm-but', 'warmer', 'warmth', 'weak', 'weight', 'well-mad',\n",
    "        'which', 'wide', 'wild', 'writ'], dtype=object)]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clfs(X_train,y_train,model='decision_tree'):\n",
    "    \"\"\"\n",
    "    In the code below, I have trained a model specifically for decision tree. You must expand the code to accommodate\n",
    "    the other two models. To learn more about sklearn's decision trees, see https://scikit-learn.org/stable/modules/tree.html\n",
    "    :param X_train: self-explanatory\n",
    "    :param y_train:\n",
    "    :param model: we will allow three values for model namely 'decision_tree', 'naive_bayes' and 'linear_SGD_classifier'\n",
    "    (Hint: you must change the loss function in https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "    to squared_loss to correctly implement the linear classifier. For the naive bayes, the appropriate model to use\n",
    "    is the Bernoulli naive Bayes.)\n",
    "\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if model == 'decision_tree':\n",
    "        clf = tree.DecisionTreeClassifier(random_state = 42)\n",
    "        clf = clf.fit(X_train, y_train)\n",
    "        return clf\n",
    "    if model == 'naive_bayes':\n",
    "        clf = BernoulliNB()\n",
    "        clf = clf.fit(X_train, y_train)\n",
    "        return clf\n",
    "    if model == 'linear_SGD_classifier':\n",
    "        clf = SGDClassifier(loss = 'squared_loss', random_state = 42)\n",
    "        clf = clf.fit(X_train, y_train)\n",
    "        return clf\n",
    "    if model == 'SVC':\n",
    "        clf = SVC(random_state = 42)\n",
    "        clf = clf.fit(X_train, y_train)\n",
    "        return clf\n",
    "    if model == 'random_forest':\n",
    "        clf = RandomForestClassifier(random_state = 42)\n",
    "        clf = clf.fit(X_train, y_train)\n",
    "        return clf\n",
    "    else:\n",
    "        return print('no such model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = ['decision_tree', 'naive_bayes', 'linear_SGD_classifier', 'SVC', 'random_forest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1200 = pd.read_csv('1200.csv') # ground truth set\n",
    "X = df_1200['reviewText']\n",
    "y = df_1200.iloc[:,1:8]\n",
    "y.columns = ['color', 'size', 'qualiti', 'comfi', 'price', 'materi', 'overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(X, y, list_words):\n",
    "    y_machine = [] # contain y_predict with all classifier\n",
    "    for clf in model_list:\n",
    "        print(clf)\n",
    "        i = 0\n",
    "        y_machine_temp = []\n",
    "        for lst in list_words: # each criterion\n",
    "            vectorizer = TfidfVectorizer(vocabulary = lst)\n",
    "#             vectorizer = TfidfVectorizer()\n",
    "            vectorizer.fit(X)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 400, random_state = 42)\n",
    "            X_train = vectorizer.transform(X_train).toarray()\n",
    "            X_test = vectorizer.transform(X_test).toarray()\n",
    "            model = train_clfs(X_train, y_train.iloc[:,i], clf)\n",
    "            y_predict = model.predict(X_test)\n",
    "            y_machine_temp.append(y_predict)\n",
    "            print(model.score(X_test, y_test.iloc[:,i]))\n",
    "            i += 1\n",
    "        y_machine.append(y_machine_temp)\n",
    "    return y_machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision_tree\n",
      "0.885\n",
      "0.5225\n",
      "0.75\n",
      "0.7525\n",
      "0.9075\n",
      "0.6825\n",
      "naive_bayes\n",
      "0.88\n",
      "0.5525\n",
      "0.755\n",
      "0.7525\n",
      "0.92\n",
      "0.745\n",
      "linear_SGD_classifier\n",
      "0.8875\n",
      "0.5425\n",
      "0.745\n",
      "0.505\n",
      "0.9075\n",
      "0.145\n",
      "SVC\n",
      "0.89\n",
      "0.5025\n",
      "0.7725\n",
      "0.775\n",
      "0.9125\n",
      "0.74\n",
      "random_forest\n",
      "0.8875\n",
      "0.51\n",
      "0.7575\n",
      "0.77\n",
      "0.905\n",
      "0.7175\n"
     ]
    }
   ],
   "source": [
    "y_machine = classify(X, y, list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dct = {} # prediction with differen classifier\n",
    "i = 0\n",
    "for clf in model_list:\n",
    "    predict_dct[clf] = pd.DataFrame(np.array(y_machine[i]).T, columns = category)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eva_metrics(y_predict, y_test):\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    for i in range(y_predict.shape[1]):\n",
    "        accuracy.append(accuracy_score(y_test.iloc[:,i], y_predict.iloc[:,i]))\n",
    "        precision.append(precision_score(y_test.iloc[:,i], y_predict.iloc[:,i], average = 'weighted')) # order: -1, 0, 1\n",
    "        recall.append(recall_score(y_test.iloc[:,i], y_predict.iloc[:,i], average = 'weighted'))\n",
    "        f1.append(f1_score(y_test.iloc[:,i], y_predict.iloc[:,i], average = 'weighted'))\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 400, random_state = 42)\n",
    "accuracy, precision, recall, f1 = eva_metrics(predict_dct['SVC'], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_lst = []\n",
    "precision_lst = []\n",
    "recall_lst = []\n",
    "f1_lst = []\n",
    "for clf in model_list:\n",
    "    accuracy, precision, recall, f1 = eva_metrics(predict_dct[clf], y_test)\n",
    "    accuracy_lst.append(accuracy)\n",
    "    precision_lst.append(precision)\n",
    "    recall_lst.append(recall)\n",
    "    f1_lst.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(accuracy_lst, columns = category, index = model_list).to_csv('accuracy.csv')\n",
    "pd.DataFrame(precision_lst, columns = category, index = model_list).to_csv('precision.csv')\n",
    "pd.DataFrame(recall_lst, columns = category, index = model_list).to_csv('recall.csv')\n",
    "pd.DataFrame(f1_lst, columns = category, index = model_list).to_csv('f1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_dict = {}\n",
    "for i in category:\n",
    "    weight = y_test[i].value_counts(normalize = True)\n",
    "    random_score = random.choices([0,1,-1],weights = [weight.iloc[0],weight.iloc[1],weight.iloc[2]],k=400)\n",
    "    random_dict[i] = random_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndm = pd.DataFrame(random_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_rndm, precision_rndm, recall_rndm, f1_rndm = eva_metrics(rndm, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.7525, 0.3925, 0.63, 0.59, 0.6725, 0.6125],\n",
       " [0.7384128148691612,\n",
       "  0.38933918767857606,\n",
       "  0.6158750000000001,\n",
       "  0.5882926829268292,\n",
       "  0.675349939338793,\n",
       "  0.616158836689038],\n",
       " [0.7525, 0.3925, 0.63, 0.59, 0.6725, 0.6125],\n",
       " [0.7453684557030209,\n",
       "  0.39083011095158127,\n",
       "  0.62285725589492,\n",
       "  0.5891393442622951,\n",
       "  0.6739118378275005,\n",
       "  0.6143201476184984])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_rndm, precision_rndm, recall_rndm, f1_rndm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random 10 times\n",
    "accuracy_rndm_list = []\n",
    "precision_rndm_list = []\n",
    "recall_rndm_list = []\n",
    "f1_rndm_list = []\n",
    "for _ in range(10):\n",
    "    random_ditc = {}\n",
    "    for i in category:\n",
    "        weight = y_test[i].value_counts(normalize = True)\n",
    "        random_score = random.choices([0,1,-1],weights = [weight.iloc[0],weight.iloc[1],weight.iloc[2]],k=400)\n",
    "        random_dict[i] = random_score\n",
    "    rndm = pd.DataFrame(random_dict)\n",
    "    accuracy_rndm, precision_rndm, recall_rndm, f1_rndm = eva_metrics(rndm, y_test)\n",
    "    accuracy_rndm_list.append(accuracy_rndm)\n",
    "    precision_rndm_list.append(precision_rndm)\n",
    "    recall_rndm_list.append(recall_rndm)\n",
    "    f1_rndm_list.append(f1_rndm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.91, 0.545, 0.7975, 0.76, 0.9125, 0.7075]\n",
      "[0.9025, 0.525, 0.7725, 0.7775, 0.905, 0.695]\n",
      "[0.8875, 0.5375, 0.7775, 0.7525, 0.9025, 0.7475]\n",
      "[0.885, 0.53, 0.7975, 0.7525, 0.8975, 0.7275]\n",
      "[0.905, 0.5575, 0.7675, 0.7725, 0.8925, 0.73]\n",
      "[0.88, 0.57, 0.7775, 0.7625, 0.865, 0.7275]\n",
      "[0.905, 0.555, 0.7475, 0.7475, 0.9, 0.745]\n",
      "[0.8725, 0.575, 0.795, 0.7675, 0.9225, 0.7225]\n",
      "[0.8775, 0.53, 0.7825, 0.755, 0.9075, 0.7325]\n",
      "[0.9125, 0.59, 0.75, 0.7575, 0.8975, 0.695]\n"
     ]
    }
   ],
   "source": [
    "# SVC 10 times\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "for rs in range(10):\n",
    "    y_machine_temp = []\n",
    "    i = 0\n",
    "    for lst in list_words: # each criterion\n",
    "        vectorizer = TfidfVectorizer(vocabulary = lst)\n",
    "#         vectorizer = TfidfVectorizer()\n",
    "        vectorizer.fit(X)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 400, random_state = rs)\n",
    "        X_train = vectorizer.transform(X_train).toarray()\n",
    "        X_test = vectorizer.transform(X_test).toarray()\n",
    "#         clf = RandomForestClassifier()\n",
    "        clf = SVC()\n",
    "        clf.fit(X_train, y_train.iloc[:,i])\n",
    "        y_predict = clf.predict(X_test)\n",
    "        y_machine_temp.append(y_predict)\n",
    "        i += 1\n",
    "    y_predict = pd.DataFrame(np.array(y_machine_temp).T)\n",
    "    accuracy, precision, recall, f1 = eva_metrics(y_predict, y_test)\n",
    "    print(accuracy)\n",
    "    accuracy_list.append(accuracy)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to csv\n",
    "pd.DataFrame(accuracy_rndm_list, columns = category).to_csv('accuracy_rndm.csv')\n",
    "pd.DataFrame(accuracy_list, columns = category).to_csv('accuracy.csv')\n",
    "pd.DataFrame(precision_rndm_list, columns = category).to_csv('precision_rndm.csv')\n",
    "pd.DataFrame(precision_list, columns = category).to_csv('precision.csv')\n",
    "pd.DataFrame(recall_rndm_list, columns = category).to_csv('recall_rndm.csv')\n",
    "pd.DataFrame(recall_list, columns = category).to_csv('recall.csv')\n",
    "pd.DataFrame(f1_rndm_list, columns = category).to_csv('f1_rndm.csv')\n",
    "pd.DataFrame(f1_list, columns = category).to_csv('f1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
